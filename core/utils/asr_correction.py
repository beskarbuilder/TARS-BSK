# ===============================================
# ASR CORRECTION ‚Äì EL FANTASMA DE LAS SIGLAS SUELTAS (ESPA√ëOL)
# ===============================================
#
# ‚ö†Ô∏è DESACTIVADO ‚Äì El parche funcionaba... pero era un arma de doble filo.
#
# PROBLEMA FUNDAMENTAL:
# - El modelo de Vosk para espa√±ol interpreta siglas como letras separadas.
# - Ejemplos t√≠picos:
#     ‚Ä¢ "SD" ‚Üí "ese de"
#     ‚Ä¢ "USB" ‚Üí "u ese be"
# - Nada grave, solo un rasgo com√∫n en sistemas ASR multiling√ºes.
#
# PERO:
# - El parche correg√≠a cosas que no deb√≠a.
# - Pod√≠a convertir una frase v√°lida en un dialecto √©lfico accidental.
# - A√±ad√≠a complejidad innecesaria a TARS (que ya tiene bastantes traumas).
#
# DECISI√ìN:
# - Parche desactivado.
# - Mejor mantener la simplicidad que a√±adir ambig√ºedad disfrazada de ayuda.
#
# A FUTURO:
# - No descartar mejoras, pero no con pegamento fon√©tico de √∫ltima hora.
# - Si llega una soluci√≥n, vendr√° bien dise√±ada (mentira: ser√°n 300 l√≠neas de regex y esperanza), no improvisada.
#
# CONCLUSI√ìN:
# - Si TARS dice ‚Äúese de‚Äù, no es un error. Es... estilo.
#
# ===============================================

# ===============================================
# 1. CONFIGURACI√ìN INICIAL Y DEPENDENCIAS
# ===============================================
# utils/asr_correction.py

import re
import logging
from typing import List, Optional, Tuple

logger = logging.getLogger("TARS.ASR")

# ===============================================
# 2. CLASE PRINCIPAL DE CORRECCI√ìN ASR
# ===============================================

class ASRCorrector:
    """
    Corrector fon√©tico para errores comunes de ASR en espa√±ol.
    Basado en principios ling√º√≠sticos y fon√©ticos, no en diccionarios.
    """
    
    # =======================
    # 2.1 INICIALIZACI√ìN
    # =======================
    def __init__(self):
        """
        Inicializa el corrector ASR con reglas fon√©ticas del espa√±ol.
        Define confusiones fon√©ticas comunes y patrones sil√°bicos.
        """
        # Caracter√≠sticas fon√©ticas del espa√±ol que causan confusiones en ASR
        self.phonetic_confusions = [
            # 1. Confusiones conson√°nticas comunes
            ('b', 'v'),  # Betacismo - mismo sonido en espa√±ol
            ('s', 'z'),  # Seseo - mismo sonido en muchas regiones
            ('y', 'll'),  # Ye√≠smo - mismo sonido en muchas regiones
            ('g', 'j'),  # Sonidos similares ante 'e', 'i'
            ('h', ''),   # H muda del espa√±ol
            ('qu', 'k'),  # Mismo sonido
            ('c', 'k'),  # Mismo sonido ante a, o, u
            ('c', 's'),  # Mismo sonido ante e, i (en seseo)
            
            # 2. Confusiones voc√°licas comunes
            ('e', 'i'),  # Vocales cerradas anteriores
            ('o', 'u'),  # Vocales cerradas posteriores
            
            # 3. Grupos problem√°ticos
            ('ue', 'g√ºe'),  # Diptongo con sonido similar
            ('bue', 'g√ºe'),  # Diptongo con sonido similar
            ('gue', 'ge'),   # Con u muda/no muda
            ('gui', 'gi'),   # Con u muda/no muda
        ]
        
        # Patrones para cierre y apertura de s√≠labas
        self.syllable_patterns = [
            # Divisi√≥n err√≥nea entre consonante-vocal
            (r'([bcdfghjklmnpqrstvwxyz])\s+([aeiou√°√©√≠√≥√∫√º])', r'\1\2'),
            # Divisi√≥n err√≥nea entre vocal-consonante
            (r'([aeiou√°√©√≠√≥√∫√º])\s+([bcdfghjklmnpqrstvwxyz][aeiou√°√©√≠√≥√∫√º])', r'\1\2'),
        ]
    
    # =======================
    # 2.2 FUNCIONES AUXILIARES
    # =======================
    def _apply_phonetic_rules(self, word: str) -> List[str]:
        """
        Genera variantes fon√©ticas probables seg√∫n reglas del espa√±ol.
        Busca corregir errores t√≠picos que el ASR podr√≠a cometer.
        Limita sustituciones para evitar cambios excesivos.
        
        Args:
            word: Palabra a analizar
            
        Returns:
            Lista de variantes fon√©ticas probables
        """
        if len(word) <= 2:  # Para palabras muy cortas, no aplicamos reglas
            return [word]
            
        variants = [word]
        new_word = word.lower()
        
        # 1. Generar variantes con confusiones fon√©ticas
        for orig, repl in self.phonetic_confusions:
            if orig in new_word:
                # Solo reemplazar la primera ocurrencia para evitar cambios excesivos
                variant = new_word.replace(orig, repl, 1)
                if variant != new_word and variant not in variants:
                    variants.append(variant)
            
            # Tambi√©n probar la confusi√≥n en sentido inverso (solo primera ocurrencia)
            if repl in new_word and repl != '':
                variant = new_word.replace(repl, orig, 1)
                if variant != new_word and variant not in variants:
                    variants.append(variant)
        
        # 2. Manejar la 'h' muda (a√±adir/quitar al inicio)
        if new_word.startswith('h'):
            variants.append(new_word[1:])
        elif new_word.startswith(('a', 'e', 'i', 'o', 'u')):
            variants.append('h' + new_word)
            
        # 3. Manejar reduplicaci√≥n de sonidos (solo primera ocurrencia)
        for consonant in 'bcdfgklmnpqrstvxz':
            double_cons = consonant*2
            if double_cons in new_word:  # Duplicada
                variant = new_word.replace(double_cons, consonant, 1)
                if variant not in variants:
                    variants.append(variant)
            elif consonant in new_word:  # No duplicada
                # Solo duplicar si no est√° al principio o final (menos probable)
                pattern = f"(?<=[aeiou√°√©√≠√≥√∫√º]){consonant}(?=[aeiou√°√©√≠√≥√∫√º])"
                variant = re.sub(pattern, double_cons, new_word, count=1)
                if variant != new_word and variant not in variants:
                    variants.append(variant)
                
        # 4. Para casos como "aser" -> "hacer" (h inicial)
        if new_word.startswith(('as', 'ac', 'ab', 'ay')):
            variants.append('h' + new_word)
        
        # 5. Aplicar m√∫ltiples reglas encadenadas (variante completa)
        full_variant = new_word
        for orig, repl in self.phonetic_confusions:
            full_variant = full_variant.replace(orig, repl)
        if full_variant != new_word and full_variant not in variants:
            variants.append(full_variant)
        
        # Filtrar variantes para mantener solo las que cambian menos caracteres
        # Cuanto m√°s grande es la palabra, m√°s importantes son estos filtros
        if len(word) > 5:
            # Calcular "distancia" entre cada variante y la palabra original
            def char_difference(s1, s2):
                # Contar caracteres diferentes entre dos cadenas
                if len(s1) != len(s2):
                    return max(len(s1), len(s2))  # Penalizar diferencia de longitud
                return sum(c1 != c2 for c1, c2 in zip(s1, s2))
            
            # Ordenar por menor n√∫mero de cambios y seleccionar las 3 mejores
            variants = sorted(variants, key=lambda v: char_difference(word, v))
            variants = variants[:min(3, len(variants))]
        
        # Eliminar duplicados preservando el orden
        variants = list(dict.fromkeys(variants))
            
        return variants
    
    def _fix_fragmentation(self, text: str) -> str:
        """
        Corrige problemas de fragmentaci√≥n donde las palabras 
        se dividen incorrectamente por el ASR.
        
        Args:
            text: Texto con posibles fragmentaciones
            
        Returns:
            Texto con fragmentaciones corregidas
        """
        # 1. Corregir fragmentaci√≥n de letras individuales (CORREGIDO)
        # Este patr√≥n une letras individuales que deber√≠an formar una palabra
        for length in range(3, 1, -1):  # Cambiado: ahora va de 3 a 2 letras para evitar problemas
            pattern = r'\b' + r'\s+'.join([r'([a-z√°√©√≠√≥√∫√º√±])'] * length) + r'\b'
            # Aseguramos que no haya referencias a grupos no existentes
            replacement = ''
            for i in range(length):
                replacement += f'\\{i+1}'
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
            
        # 2. Aplicar patrones sil√°bicos
        for pattern, replacement in self.syllable_patterns:
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
            
        # 3. Corregir palabras truncadas comunes
        text = re.sub(r'\b([ptksbd])\s+([aeiou√°√©√≠√≥√∫√º])\b', r'\1\2', text, flags=re.IGNORECASE)
        
        # 4. Unir n√∫meros fragmentados (como "tres p o" -> "3po")
        # Corregido: ahora usamos grupos en el patr√≥n y referenciamos correctamente
        text = re.sub(r'\buno\s+([a-z√°√©√≠√≥√∫√º√±])\b', r'1\1', text, flags=re.IGNORECASE)
        text = re.sub(r'\bdos\s+([a-z√°√©√≠√≥√∫√º√±])\b', r'2\1', text, flags=re.IGNORECASE)
        text = re.sub(r'\btres\s+([a-z√°√©√≠√≥√∫√º√±])\b', r'3\1', text, flags=re.IGNORECASE)
        text = re.sub(r'\bcuatro\s+([a-z√°√©√≠√≥√∫√º√±])\b', r'4\1', text, flags=re.IGNORECASE)
        
        # 5. Corregir f√≥rmulas y notaciones cient√≠ficas
        if self._has_fragmentation(text):
            logger.debug("üß© Fragmentaci√≥n detectada, aplicando correcci√≥n.")
            text = self._fix_fragmentation(text)
        else:
            logger.debug("‚úÖ Sin fragmentaci√≥n aparente, no se aplica fusi√≥n.")

        
        return text
    
    def _fix_scientific_notation(self, text: str) -> str:
        """
        Corrige notaci√≥n cient√≠fica y f√≥rmulas mal reconocidas.
        Utiliza reglas generales, no patrones espec√≠ficos.
        
        Args:
            text: Texto con posibles notaciones cient√≠ficas
            
        Returns:
            Texto con notaciones cient√≠ficas corregidas
        """
        # Convertir representaciones verbales a notaci√≥n simb√≥lica
        
        # 1. Elementos qu√≠micos comunes
        text = re.sub(r'\bace\b|\bh\s*ace\b', 'H', text, flags=re.IGNORECASE)
        
        # 2. N√∫meros seguidos de letras (posible notaci√≥n cient√≠fica)
        text = re.sub(r'\b(uno|un)\s+([a-z])\b', r'1\2', text, flags=re.IGNORECASE)
        text = re.sub(r'\bdos\s+([a-z])\b', r'2\1', text, flags=re.IGNORECASE)  # Corregido: \2 ‚Üí \1
        text = re.sub(r'\btres\s+([a-z])\b', r'3\1', text, flags=re.IGNORECASE)  # Corregido: \2 ‚Üí \1
        text = re.sub(r'\bcuatro\s+([a-z])\b', r'4\1', text, flags=re.IGNORECASE)  # Corregido: \2 ‚Üí \1
        
        # 3. Patr√≥n general para letras de f√≥rmulas (e.g. "h tres p o" -> "H3PO")
        elements = ['h', 'c', 'o', 'n', 'p', 's', 'k', 'ca', 'na', 'cl', 'fe', 'mg']
        for elem in elements:
            # Buscar el patr√≥n "elem + n√∫mero + letra"
            pattern = fr'\b{elem}\s+(uno|un|dos|tres|cuatro|cinco)\s+([a-z√°√©√≠√≥√∫√º]{{1,2}})\b'
            
            # Funci√≥n para reemplazar n√∫meros en palabras con d√≠gitos
            def convert_number(match):
                number_word = match.group(1).lower()
                letter = match.group(2)
                
                numbers = {'uno': '1', 'un': '1', 'dos': '2', 'tres': '3', 
                           'cuatro': '4', 'cinco': '5'}
                
                return f"{elem.upper()}{numbers.get(number_word, '')}{letter.upper()}"
                
            text = re.sub(pattern, convert_number, text, flags=re.IGNORECASE)
        
        return text
    
    def _normalize_text(self, text: str) -> str:
        """
        Normaliza el texto para eliminar caracteres extra√±os y espacios m√∫ltiples.
        
        Args:
            text: Texto a normalizar
            
        Returns:
            Texto normalizado
        """
        # Mantener letras, n√∫meros, espacios y signos de puntuaci√≥n b√°sicos
        text = re.sub(r'[^\w\s\.,;:¬ø?¬°!√°√©√≠√≥√∫√º√±√Å√â√ç√ì√ö√ú√ë-]', '', text)
        
        # Normalizar espacios m√∫ltiples
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _fix_special_commands(self, text: str) -> str:
        """
        Detecta y corrige comandos esenciales como wakewords 
        utilizando principios ac√∫sticos y fon√©ticos.
        
        Args:
            text: Texto con posible comando wakeword
            
        Returns:
            Texto con comando wakeword corregido si es necesario
        """
        # Detectar posible "oye tars" por principios fon√©ticos
        # 1. Variaciones del comando "oye" (ac√∫sticamente similares)
        first_word = text.split()[0].lower() if text and ' ' in text else ''
        if first_word:
            # Detectar variaciones de "oye" 
            if any(similar in first_word for similar in ['oye', 'oie', 'olle', 'hoye', 'oi', 'oy']):
                # Detectar variaciones de "tars" en la segunda palabra
                parts = text.split(maxsplit=1)
                if len(parts) > 1:
                    rest = parts[1].lower()
                    first_rest = rest.split()[0] if ' ' in rest else rest
                    
                    # Funci√≥n de similitud fon√©tica simplificada
                    def similarity_with_tars(word):
                        target = 'tars'
                        # Eliminar caracteres no alfab√©ticos
                        word = re.sub(r'[^a-z√°√©√≠√≥√∫√º√±]', '', word.lower())
                        
                        # Si es muy diferente en longitud, baja similitud
                        if abs(len(word) - len(target)) > 2:
                            return 0.0
                            
                        # Comprobar coincidencias de consonantes (m√°s importantes)
                        target_consonants = [c for c in target if c not in 'aeiou']
                        word_consonants = [c for c in word if c not in 'aeiou√°√©√≠√≥√∫√º']
                        
                        # Coincidencia de consonantes
                        consonant_match = sum(1 for c in word_consonants if c in target_consonants)
                        consonant_score = consonant_match / max(len(target_consonants), len(word_consonants)) if max(len(target_consonants), len(word_consonants)) > 0 else 0
                        
                        # Coincidencia general de caracteres
                        common = set(word) & set(target)
                        general_score = len(common) / max(len(set(word)), len(set(target))) if max(len(set(word)), len(set(target))) > 0 else 0
                        
                        # Combinaci√≥n ponderada
                        return (consonant_score * 0.7) + (general_score * 0.3)
                    
                    # Si hay alta similitud con "tars", corregir todo el comando
                    similarity = similarity_with_tars(first_rest)
                    if similarity > 0.5:
                        corrected_rest = ' '.join(['TARS'] + rest.split()[1:]) if ' ' in rest else 'TARS'
                        logger.info(f"üîÑ Detectado comando wakeword: '{text}' ‚Üí 'oye {corrected_rest}'")
                        return f"oye {corrected_rest}"
        
        return text
    
    # =======================
    # 2.3 FUNCI√ìN PRINCIPAL DE CORRECCI√ìN
    # =======================
    def correct_text(self, text: str) -> str:
        """
        Aplica correcciones solo si hay indicios claros de errores ASR.
        No modifica frases correctamente estructuradas.
        
        Args:
            text: Texto a corregir
            
        Returns:
            Texto corregido
        """
        if not text or len(text.strip()) == 0:
            return text

        original = text
        text = self._normalize_text(text)
        text = self._fix_special_commands(text)
        text = self._fix_scientific_notation(text)

        # ‚ö†Ô∏è SOLO aplicar fragmentaci√≥n si se detecta
        if self._has_fragmentation(text):
            logger.debug("üß© Fragmentaci√≥n detectada, aplicando fusi√≥n de palabras...")
            text = self._fix_fragmentation(text)
        else:
            logger.debug("‚úÖ Sin fragmentaci√≥n detectada, no se toca el texto.")

        # Palabras individuales sospechosas (correcciones menores)
        words = text.split()
        corrected_words = []

        for word in words:
            if len(word) <= 3 or word.isdigit():
                corrected_words.append(word)
                continue

            improbable_clusters = ['cz', 'gk', 'js', 'pb', 'tp', 'kp', 'tn', 'nm', 'dzs']
            if any(cluster in word.lower() for cluster in improbable_clusters):
                variants = self._apply_phonetic_rules(word)
                corrected_word = next((v for v in variants if v != word), word)
                corrected_words.append(corrected_word)
            else:
                corrected_words.append(word)

        corrected_text = ' '.join(corrected_words)

        if corrected_text != original:
            logger.info(f"üîÑ ASR Corregido: '{original}' ‚Üí '{corrected_text}'")

        return corrected_text

    # =======================
    # 2.4 DETECTOR DE FRAGMENTACI√ìN
    # =======================
    def _has_fragmentation(self, text: str) -> bool:
        """
        Detecta si la entrada contiene fragmentaci√≥n de palabras (e.g. 't e p a r e c e').
        
        Args:
            text: Texto a analizar
            
        Returns:
            bool: True si se detecta fragmentaci√≥n
        """
        if not text:
            return False

        if re.search(r'\b([a-z√°√©√≠√≥√∫√º√±])(\s+[a-z√°√©√≠√≥√∫√º√±]){2,}\b', text, re.IGNORECASE):
            return True

        words = text.split()
        if len(words) >= 4 and sum(len(w) <= 2 for w in words) >= 3:
            return True

        return False

# ===============================================
# $ git log --format="%h %s" -1 [current_file]  
# deadbeef chore: Update [current_file] (survived again)  
# $ git blame --porcelain [current_file] | grep "exist"  
# fatal: No existential commits found
# ===============================================