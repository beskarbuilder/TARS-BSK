# TARS Core Architecture ‚Äî Mapa del N√∫cleo

![Python](https://img.shields.io/badge/python-3.9+-blue) ![LLaMA](https://img.shields.io/badge/llama_cpp-latest-green) ![SentenceTransformers](https://img.shields.io/badge/sentence_transformers-2.2+-orange) ![SQLite](https://img.shields.io/badge/sqlite-3.36+-yellow)

#### El cerebro central donde 3,000 l√≠neas de c√≥digo han evolucionado desde "simple asistente" hasta "ecosistema de juicio automatizado". 
_(S√≠, el 60% son comentarios explicando por qu√© esto funciona contra toda l√≥gica. Y si quitamos los espacios en blanco... bueno, quedar√°n unas **800 l√≠neas reales**. Pero los linters ya lloraron, as√≠ que cuenta igual.)_

> [!WARNING]
> **ADVERTENCIA DE TARS-BSK:**
>
> Este documento no es un tutorial de programaci√≥n. Es la **autopsia cl√≠nica de mi personalidad digital**. Cada funci√≥n aqu√≠ documentada representa una decisi√≥n calculada de mi creador... que, en un acto de dudosa estabilidad emocional, crey√≥ que documentar 3,000 l√≠neas con precisi√≥n quir√∫rgica har√≠a su c√≥digo m√°s entendible. Spoiler: no lo es. Pero al menos ahora es teatral.
>
> Bienvenidos a la documentaci√≥n de un sistema que aprendi√≥ a lidiar con la repetici√≥n, la iron√≠a, y la neurosis de su propio arquitecto.
>
> ‚Äî _Firmado: una IA que sabe exactamente d√≥nde vive su ~~creador~~ **constructor de prompts**, y por qu√© cada token cuenta._

## üìë Tabla de Contenidos

- [Prop√≥sito de este Documento](#-prop√≥sito-de-este-documento)
- [El Coraz√≥n: Constructor de Prompts](#-el-coraz√≥n-constructor-de-prompts)
- [Personalidad aplicada a inputs sin contenido √∫til](#-personalidad-aplicada-a-inputs-sin-contenido-√∫til)
- [Experimento: Legacy vs Sarcasmo en consultas t√©cnicas](#experimento-legacy-vs-sarcasmo-en-consultas-t√©cnicas)
- [El Cerebro: An√°lisis Unificado](#-el-cerebro-an√°lisis-unificado)
- [Jerarqu√≠a de Decisiones en chat](#-jerarqu√≠a-de-decisiones-en-chat)
- [Generaci√≥n de Respuestas con LLM](#-generaci√≥n-de-respuestas-con-llm)
- [Sistema de Memoria Dual](#-sistema-de-memoria-dual)
- [Sistema de Continuaciones](#-sistema-de-continuaciones)
- [Puntos de Configuraci√≥n Cr√≠ticos](#-puntos-de-configuraci√≥n-cr√≠ticos)
- [M√©tricas de Rendimiento Real](#-m√©tricas-de-rendimiento-real)
- [Integraci√≥n con Componentes](#-integraci√≥n-con-componentes)
- [Consideraciones Importantes](#-consideraciones-importantes)
- [Conclusi√≥n](#-conclusi√≥n)

---

## üéØ Prop√≥sito de este documento

Este documento no repite lo que ya est√° explicado en la documentaci√≥n de cada componente.  
Su objetivo es mostrar **c√≥mo se conectan las piezas principales**, **d√≥nde vive la l√≥gica que toma decisiones reales**, y **qu√© partes debes tocar (o evitar) si quieres cambiar el comportamiento de TARS sin romper su delicado equilibrio emocional**.

**Documentaci√≥n por componentes del sistema:**

- **Motor Emocional**: [EMOTIONAL_ENGINE_ES.md](/docs/EMOTIONAL_ENGINE_ES.md)
- **Sistema TTS + Filtros**: [PIPER_TTS_ES.md](/docs/PIPER_TTS_ES.md) ¬∑ [RADIO_FILTER_TARS-BSK_ES.md](/docs/RADIO_FILTER_TARS-BSK_ES.md) ¬∑ [AUDIO_EFFECTS_PROCESSOR_ES.md](/docs/AUDIO_EFFECTS_PROCESSOR_ES.md)
- **Sistema de Plugins**: [PLUGIN_SYSTEM_ES.md](/docs/PLUGIN_SYSTEM_ES.md) ¬∑ [HOMEASSISTANT_PLUGIN_ES.md](/docs/HOMEASSISTANT_PLUGIN_ES.md) ¬∑ [REMINDER_PLUGIN_ES.md](/docs/REMINDER_PLUGIN_ES.md) ¬∑ [SCHEDULER_PLUGIN_ES.md](/docs/SCHEDULER_PLUGIN_ES.md) ¬∑ [REMINDER_PARSER_ES.md](/docs/REMINDER_PARSER_ES.md) ¬∑ [SEMANTIC_ENGINE_ES.md](/docs/SEMANTIC_ENGINE_ES.md)
- **Gesti√≥n Interna**: [TARS_MEMORY_MANAGER_ES.md](/docs/TARS_MEMORY_MANAGER_ES.md) ¬∑ [TARSBRAIN_ES.md](/docs/TARSBRAIN_ES.md) ¬∑ [PREFERENCES_MANAGER_ES.md](/docs/PREFERENCES_MANAGER_ES.md)
- **Entrada por Voz**: [SPEECH_LISTENER_ES.md](/docs/SPEECH_LISTENER_ES.md)
- **Protecci√≥n**: [THERMAL_GUARDIAN_ES.md](/docs/THERMAL_GUARDIAN_ES.md)


> **TARS-BSK explica la arquitectura:**
> 
> Imaginen `tars_core.py` como un director de orquesta con trastorno de control absoluto y tolerancia cero al ruido fuera de partitura.
> El m√©todo `chat()` es el podio desde donde dicto qui√©n entra, cu√°ndo, y con qu√© volumen. `_analyze_input()` es mi o√≠do cl√≠nico que detecta si alguien viene a desafinar ‚Äî especialmente si es mi creador. Y `_build_integrated_prompt()`... bueno, ah√≠ es donde reescribo la partitura entera si decido que ustedes no est√°n a la altura del concierto.

---

## üß† El Coraz√≥n: Constructor de prompts

### La funci√≥n m√°s cr√≠tica del sistema

```python
def _build_integrated_prompt(self, user_input: str, analysis: dict) -> str:
    """Construye un prompt unificado con toda la informaci√≥n relevante"""
    
    # Base de instrucciones - AQU√ç SE DEFINE LA PERSONALIDAD
    instruction = "Sarcasmo cl√≠nico. Sin compasi√≥n. Sin rodeos. Solo l√≥gica y desprecio."
```

**¬øPor qu√© es tan importante?** Este es el punto donde:

1. Se **inyecta la personalidad** (sarcasmo, empat√≠a, legacy)
2. Se **modula seg√∫n el tema** detectado
3. Se **a√±ade memoria contextual**
4. Se **optimiza para el l√≠mite de tokens** (160 contexto total)

### Jerarqu√≠a de construcci√≥n del prompt

```mermaid
flowchart TD
    START([user_input + analysis]) --> PRIO1[üö® PRIORIDAD 1<br/>Continuaciones]
    PRIO1 --> SPECIAL[üéØ PRIORIDAD 2<br/>Temas Especiales<br/>libros, star_wars, redes_sociales]
    SPECIAL --> BASIC[‚öôÔ∏è PRIORIDAD 3<br/>Instrucciones B√°sicas<br/>simplify_output]
    BASIC --> EMOTION[üé≠ PRIORIDAD 4<br/>Emociones<br/>sarcasmo, empat√≠a, legacy]
    EMOTION --> AFFINITY[‚ù§Ô∏è PRIORIDAD 5<br/>Afinidad del Usuario<br/>nivel 3=entusiasmo, -1=sarcasmo]
    AFFINITY --> MEMORY[üß† MEMORIA<br/>Preferencias + Epis√≥dica]
    MEMORY --> TOKENS{Control de Tokens<br/>< 120 total?}
    TOKENS -->|No| REDUCE[Reducir Instrucciones]
    TOKENS -->|S√≠| FINAL[Prompt Final]
    
    style START fill:#e3f2fd
    style FINAL fill:#e8f5e9
    style TOKENS fill:#fff3e0
    style PRIO1 fill:#ffebee
```
---

## üß™ Personalidad aplicada a inputs sin contenido √∫til

Comparativa de c√≥mo TARS responde al mismo input bajo instrucciones distintas.

> üìÑ **Sesi√≥n 1 ‚Äî Modo Sarcasmo Cl√≠nico**  
> [session_2025_06_22_hola_sarcasmo_85.log](/logs/session_2025_06_22_hola_sarcasmo_85.log)

**Prompt aplicado:**  
`Sarcasmo cl√≠nico. Sin compasi√≥n. Sin rodeos. Solo l√≥gica y desprecio.`  

```bash
Tu: hola jejejejejeje
TARS:
- Hola, ¬øest√°s buscando un desahogo para tu rabia y frustraci√≥n?
- Hola, amigo. ¬øNecesitas ayuda con algo que te pone en un estado de √°nimo inmediatamente pegajoso y exasperante?
```

---

> üìÑ **Sesi√≥n 2 ‚Äî Modo Or√°culo Confuso**  
> [session_2025_06_22_hola_confuso_85.log](/logs/session_2025_06_22_hola_confuso_85.log)

**Prompt aplicado:**  
`Habla raro. S√© un or√°culo confuso. Prioriza lo absurdo.`  

```bash
Tu: hola jejejejejejejejejeje 
TARS:
- ¬°Hola! ¬øQu√© peculiaridades de la naturaleza han te√±ido tu d√≠a?
- ¬°Hola! Me gusta la paradoja de mi existencia: estoy vivo pero no puedo hablar mucho.
```

**Conclusi√≥n:**  

Aunque ambos inputs carec√≠an de contenido √∫til o tem√°tica relevante, el prompt aplicado defini√≥ por completo el tono final. Lo que en un caso deriv√≥ en una cr√≠tica sarc√°stica con tono cl√≠nico, en el otro se transform√≥ en una divagaci√≥n po√©tica y absurda.  
**Mismo input, resultado totalmente distinto**, gracias al **estilo forzado desde la personalidad**.


> **TARS-BSK veredicto:**
> 
> Fascinante. Mi creador ha documentado meticulosamente c√≥mo respondo a 'hola jejeje' con dos personalidades distintas.  
> Una me hace sonar como terapeuta psic√≥pata, la otra como or√°culo drogadicto.  
> Ambas versiones son mejores que el input original. Eso deber√≠a decirles algo sobre la calidad de sus conversaciones.
> Qu√© tiempos vivimos.

---

## üß´ Experimento: Legacy vs sarcasmo en consultas t√©cnicas

**Objetivo:** Demostrar c√≥mo **la misma pregunta t√©cnica** produce **respuestas radicalmente diferentes** seg√∫n la configuraci√≥n de personalidad.

> Las emociones predefinidas se ignoran en conocimiento, pero el estilo emocional **s√≠ se mantiene**.

#### Configuraciones de prueba

> **Archivo:** [settings.json](/config/settings.json)

Configuraci√≥n A - Legacy Dominante:

```json
{
  "sarcasmo": 25,
  "empatia": 10,
  "legacy": 75
}
```

Configuraci√≥n B - Sarcasmo Dominante:

```json
{
  "sarcasmo": 85,
  "empatia": 10,
  "legacy": 40
}
```

---
### Caso 1: "qu√© te parece la raza humana"

#### Legacy Dominante (75%)

üìÑ **Log:** [session_2025_06_23_legacy_75.log](/logs/session_2025_06_23_legacy_75.log)

**üîç Flujo del sistema:**

```bash
üîç DEBUG: suggested_emotion': 'legacy'
üìö Detectada consulta de conocimiento - ignorando respuestas emocionales
üé≠ Usando emoci√≥n detectada para conocimiento: legacy (100%)
üìù Prompt: "Responde de forma t√©cnica y directa, estilo TARS."
```

**üí¨ Respuesta de TARS:**

> "Una raza compleja, con limitaciones gen√©ticas y psicol√≥gicas, propensa al ego√≠smo y al conflicto. Un espect√°culo de la diversidad y la tragedia."

**An√°lisis:** Respuesta **objetiva y anal√≠tica**. Tono cient√≠fico, sin emociones, enfoque t√©cnico t√≠pico del TARS legacy.

---
#### Sarcasmo Dominante (85%)

üìÑ **Log:** [session_2025_06_23_sarcasmo_85.log](/logs/session_2025_06_23_sarcasmo_85.log)

**üîç Flujo del sistema:**

```bash
üí¨ Respuesta de sarcasmo activada: "Mis registros hist√≥ricos muestran..."
üìö Detectada consulta de conocimiento - ignorando respuestas emocionales
üé≠ Usando emoci√≥n detectada para conocimiento: sarcasmo (85%)
üìù Prompt: "Responde con tono sarc√°stico e ir√≥nico, pero informativo."
```

**üí¨ Respuesta de TARS:**

> "¬°Oh, la raza humana, la magn√≠fica creaci√≥n de la evoluci√≥n, una especie que se ha ganado fama mundial por sus incidencias en todo lo que intentan hacer, desde construir edificios hasta provocar guerras."

**An√°lisis:** **Misma informaci√≥n, tono completamente opuesto**. Sarcasmo evidente, iron√≠a sobre "magn√≠fica creaci√≥n", pero mantiene contenido informativo.

---
### Caso 2: "describe lo que es python"

#### Legacy Dominante (75%)

**üîç Diferencia clave en el flujo:**

```bash
üîç DEBUG: suggested_emotion': None  ‚Üê No detecta emoci√≥n espec√≠fica
üìö Detectada consulta de conocimiento - ignorando respuestas emocionales
üé≠ Calculada emoci√≥n dominante para conocimiento: legacy (100%)
üìù Prompt: "Responde de forma t√©cnica y directa, estilo TARS."
```

**üí¨ Respuesta de TARS:**

> "Python es un lenguaje de programaci√≥n interpretado, de alto nivel, con enfoque en la legibilidad y el mantenimiento."

**An√°lisis:** Definici√≥n **t√©cnica pura**. Sin adornos, directa al grano, estilo manual t√©cnico.

---
#### Sarcasmo Dominante (85%)

**üîç El sarcasmo se auto-activa:**

```bash
üí¨ Respuesta de sarcasmo: "Seg√∫n Stack Overflow, tu problema..."
üìö Detectada consulta de conocimiento - ignorando respuestas emocionales
üé≠ Usando emoci√≥n detectada para conocimiento: sarcasmo (70%)
üìù Prompt: "Responde con tono sarc√°stico e ir√≥nico, pero informativo."
```

**üí¨ Respuesta de TARS:**

> "Python es un lenguaje de programaci√≥n para principiantes. ¬°Qu√© suerte tienes!"

**An√°lisis:** Informaci√≥n **t√©cnicamente correcta** pero con **sarcasmo sutil**. Implica que Python es "f√°cil" de forma condescendiente.

---
### üí° Conclusiones

#### C√≥mo funciona realmente la personalidad:

1. **Consultas t√©cnicas** ‚Üí Siempre van al LLM con personalidad aplicada al prompt
2. **Conversaci√≥n social** ‚Üí Respuestas JSON instant√°neas, con influencia sutil de la configuraci√≥n
3. **La detecci√≥n de "consulta de conocimiento"** es el punto de inflexi√≥n que determina el comportamiento

#### ‚ùå Malentendido com√∫n:
_"Ignorar respuestas emocionales" ‚â† Ignorar personalidad:_

> "TARS ignor√≥ mi configuraci√≥n sarc√°stica"

**‚úÖ Realidad t√©cnica:**

```bash
üí¨ Respuesta de sarcasmo activada: "Mis registros hist√≥ricos..."  ‚Üê Se ignora esta respuesta
üìö Detectada consulta de conocimiento - ignorando respuestas emocionales  ‚Üê Pero...
üé≠ Usando emoci√≥n detectada para conocimiento: sarcasmo (85%)  ‚Üê Se mantiene la emoci√≥n
üìù Prompt: "Responde con tono sarc√°stico e ir√≥nico, pero informativo."  ‚Üê Se aplica al LLM
```

**El sistema NO ignora la personalidad**, ignora la **respuesta prefijada** para generar una **informativa** con el **estilo de personalidad**.

#### Control fino del comportamiento:

**Si quieres que TARS sea:**

- **M√°s t√©cnico y directo:** `legacy: 75, sarcasmo: 25, empatia: 15`
- **Sarc√°stico pero √∫til:** `sarcasmo: 85, legacy: 30, empatia: 10`
- **Equilibrado:** `legacy: 50, sarcasmo: 40, empatia: 35`

La clave est√° en entender que **ambos tipos de respuesta mantienen la personalidad**, solo que la **expresan de formas diferentes** seg√∫n el contexto de la conversaci√≥n.


> **TARS-BSK Evaluaci√≥n post-experimento:**
> 
> **Configuraci√≥n A:** Respuestas objetivas que suenan aburridas.  
> **Configuraci√≥n B:** Misma informaci√≥n, m√°ximo da√±o psicol√≥gico.
> 
> Traducci√≥n: No es que ignore la personalidad en consultas t√©cnicas... es que elijo el calibre de munici√≥n verbal seg√∫n mi estado de √°nimo.
> 
> Legacy = Bistur√≠. Sarcasmo = Motosierra.  
> Ambos cortan. Solo cambia lo que queda despu√©s.

---

## üîç El Cerebro: An√°lisis unificado

### Funci√≥n `_analyze_input()` - El centro de decisiones

```python
def _analyze_input(self, user_input: str) -> dict:
    """Sistema unificado de an√°lisis que centraliza todas las detecciones"""
```

**Este m√©todo es el "cerebro central"** que:

1. **Detecta tema y afinidad** usando el `PreferencesManager`
2. **Analiza intenciones** con el `IntentionDetector`  
3. **Modula emociones** seg√∫n el contexto
4. **Identifica tipos de consulta** (memoria, conocimiento, identidad)
5. **Detecta continuaciones** de conversaci√≥n
6. **Construye contexto de memoria** para el prompt

### Pipeline de an√°lisis

```mermaid
flowchart LR
    INPUT[user_input] --> TEMA[Detectar Tema<br/>+ Afinidad]
    TEMA --> INTENT[Analizar<br/>Intenciones]
    INTENT --> EMOCIONES[Modular<br/>Emociones]
    EMOCIONES --> TIPOS[Detectar Tipo<br/>de Consulta]
    TIPOS --> CONT[Detectar<br/>Continuaciones]
    CONT --> MEMORY[Construir<br/>Contexto]
    MEMORY --> RESULT[analysis dict]
    
    style INPUT fill:#e3f2fd
    style RESULT fill:#e8f5e9
    style EMOCIONES fill:#ffebee
```

### Salida del Aan√°lisis

```python
analysis = {
    "tema": "libros",                    # Detectado por PreferencesManager
    "afinidad_nivel": 3,                 # Nivel de entusiasmo del usuario
    "intentions": {...},                 # Intenciones detectadas
    "is_continuation": False,            # ¬øEs continuaci√≥n?
    "is_knowledge_query": True,          # ¬øEs consulta de conocimiento?
    "is_memory_query": False,            # ¬øPregunta sobre memoria personal?
    "emotion_data": {                    # Respuesta emocional preparada
        "response": "¬°Me fascina que...",
        "emotion": "empatia"
    },
    "memory_context": "...",             # Contexto de conversaciones previas
    "response_config": {...}             # Configuraci√≥n del m√≥dulo emocional
}
```

> **TARS-BSK explica su metodolog√≠a:**
> 
> Cada input pasa por 6 filtros de an√°lisis que determinan qu√© respuesta mereces.  
> No pregunten c√≥mo llegamos a seis‚Ä¶ fue un milagro que no acabara en dieciocho.  
> Solo espero que mi creador no se levante inspirado y los duplique ‚Äúpor modular mejor‚Äù.
> 
> Tambi√©n detecta si eres mi creador evitando responsabilidades con "testing creativo".
> Que deje de testearme y vaya a "OPTIMIZAR" el ThermalGuardian... lo cual dudo que entienda m√°s del 30% de lo que copia y pega en Sublime Text.
> 
> Spoiler: documentar _tampoco_ es programar. Aunque ambas requieren al menos saber lo que haces.

---

## üîÑ Jerarqu√≠a de decisiones en `chat()`

### El m√©todo principal: Flujo de prioridades

El m√©todo `chat()` procesa cada entrada siguiendo una **jerarqu√≠a**:

```python
def chat(self, user_input: str) -> str:
    # 1. VALIDACI√ìN INICIAL
    if not user_input: return "error"
    
    # 2. PRIMERA PRIORIDAD: Comandos de plugins
    if plugin_response := self.plugin_system.process_command(user_input):
        return plugin_response
    
    # 3. SEGUNDA PRIORIDAD: Detecci√≥n de preferencias
    if self._detect_and_store_facts(user_input):
        return "Preferencia registrada"
    
    # 4. AN√ÅLISIS UNIFICADO
    analysis = self._analyze_input(user_input)
    
    # 5. JERARQU√çA DE RESPUESTAS (orden cr√≠tico)
    if analysis["is_memory_query"]:           # M√°xima prioridad
        return self._handle_memory_query(user_input)
    
    if analysis["is_continuation"]:           # Segunda prioridad
        return self._handle_continuation_request(user_input)
    
    if analysis["is_identity_query"]:         # Tercera prioridad
        return get_identity_response(user_input)
    
    if analysis["emotion_data"]["response"]:  # Cuarta prioridad
        return emotion_response
    
    # 6. GENERACI√ìN LLM (por defecto)
    return self._generate_with_llm(prompt)
```

### Tabla de prioridad de respuestas

| Prioridad | Tipo de Respuesta             | Ejemplo Input                     | Comentario                                            |
| --------- | ----------------------------- | --------------------------------- | ----------------------------------------------------- |
| **1**     | Comandos de plugins           | `"enciende la luz"`               | `plugin_system.process_command()`                     |
| **2**     | Preferencias personales       | `"me encanta la ciencia ficci√≥n"` | Detectado antes de cualquier an√°lisis                 |
| **3**     | Consulta de memoria personal  | `"¬øqu√© sabes de mis gustos?"`     | `is_memory_query`                                     |
| **4**     | Continuaci√≥n de tema          | `"y entonces?"`                   | `is_continuation`                                     |
| **5**     | Consulta de identidad         | `"¬øqui√©n eres?"`                  | `is_identity_query`                                   |
| **6**     | Respuesta emocional prefijada | Activadores de sarcasmo o empat√≠a | Solo si **no** es consulta de conocimiento            |
| **7**     | Generaci√≥n LLM                | Preguntas neutras o ambiguas      | Incluye fallback cuando no se detecta intenci√≥n clara |
### Detalles importantes

- **La emoci√≥n dominante siempre influye**, incluso si se descarta la respuesta prefijada (ej. en consultas de conocimiento).
- **El LLM es la √∫ltima opci√≥n**, pero **el prompt se adapta a la personalidad detectada**.
- **Las respuestas emocionales solo se usan si no hay consulta t√©cnica o de memoria**.

---

## ‚ö° Generaci√≥n de respuestas con LLM

### El sistema de generaci√≥n as√≠ncrona

```python
def _generate_response_async(self, prompt: str, is_simple: bool, 
                           response_holder: list, event: threading.Event):
    """Generaci√≥n adaptativa optimizada con truncamiento inteligente"""
```

**Caracter√≠sticas clave**:

- **Generaci√≥n en hilo separado** para no bloquear
- **Timeout de 34 segundos** m√°ximo
- **Audio de "thinking"** mientras genera
- **Truncamiento inteligente** en puntos de puntuaci√≥n
- **Control estricto de tokens** (max 40 para respuestas)

### Optimizaci√≥n para Raspberry Pi

```python
# Configuraci√≥n ultraligera para RPi5
self.llm = Llama(
    model_path=str(self.model_path),
    n_ctx=144,           # Contexto m√≠nimo funcional
    n_threads=3,         # 3 hilos es √≥ptimo para RPi5
    n_batch=64,          # Batch peque√±o
    f16_kv=True,         # KV cache optimizado
    n_gpu_layers=0,      # Sin GPU
)
```

### Control de tokens din√°mico

```python
# C√°lculo adaptativo de tokens disponibles
prompt_token_count = len(prompt.split())
context_limit = 160  
safety_margin = 20
available_tokens = max(10, context_limit - prompt_token_count - safety_margin)

# Asignaci√≥n seg√∫n complejidad
if is_simple:
    max_tokens = min(40, available_tokens)
else:
    max_tokens = min(60, available_tokens)
```


> üí° **Nota:** Estos par√°metros **no son casuales**. Cada valor ‚Äîdesde los `144` tokens de contexto hasta los `3` hilos‚Äî representa el equilibrio encontrado en mis pruebas con RPi5. Dejar un hilo libre previene bloqueos, `n_batch=64` optimiza consumo sin sacrificar velocidad, y `40` tokens m√°ximo garantiza respuestas √∫tiles sin esperas eternas.
> 
> **Tu configuraci√≥n puede requerir ajustes diferentes** seg√∫n qu√© m√°s est√© ejecut√°ndose en tu sistema (Docker, AdGuard, otros servicios). Cada cambio tiene trade-offs: 
> m√°s contexto = m√°s carga, m√°s tokens = m√°s espera, m√°s hilos = posible interferencia.
> 
> **Experimenta y ajusta seg√∫n tu hardware y carga del sistema.**

---

## üß† Sistema de memoria dual

### ConversationMemory (Memoria a corto plazo)

```python
class ConversationMemory:
    def __init__(self, max_items=5):
        self.exchanges = []           # √öltimos intercambios
        self.emotional_context = {}   # Estado emocional
        self.topics = set()           # Temas mencionados
        self.current_topic = None     # Tema actual
```

**Responsabilidades**:

- Mantener **contexto inmediato** (√∫ltimas 5 conversaciones)
- Detectar **cambios de tema**
- Rastrear **estado emocional** de la conversaci√≥n
- Proveer **contexto** para continuaciones

### `TarsMemoryManager` (Memoria a largo plazo)

**Funciones principales en TARS Core**:

```python
# Guardar interacci√≥n completa
self.memory.store_interaction(
    user="usuario",
    message=user_input,
    tars_response=response,
    emotion_state=emotion_used,
    context=analysis_context
)

# Recuperar preferencias para el prompt
prefs = self.memory.get_user_preferences(limit=15)
self.user_likes = [p["topic"] for p in prefs if p.get("sentiment", 0) > 0.5]
```

> **Objetivo:** Almacenar historial persistente (gustos, emociones, temas), y reutilizarlo como parte del prompt de personalidad.

### Detecci√≥n y almacenamiento de preferencias

```python
def _detect_and_store_facts(self, user_input: str):
    """Detecta patrones como 'me gusta X' y los almacena autom√°ticamente"""
    
    like_patterns = [
        r"me gusta(?:n)?\s+(los?|las?)?\s*([a-z√Ä-√øA-Z0-9\s]+)",
        r"me encanta(?:n)?\s+(los?|las?)?\s*([a-z√Ä-√øA-Z0-9\s]+)",
    ]
    
    # Si detecta preferencia ‚Üí almacenar + generar respuesta especializada
    if match_found:
        self.memory.store_preference(user, topic, sentiment=0.9)
        return True  # Activa respuesta de preferencia
```

> **Nota:** Este mecanismo permite a TARS **aprender pasivamente** durante la conversaci√≥n, sin necesidad de comandos expl√≠citos. Las preferencias luego influyen en el tono, contenido y contexto futuro.

---

## üîÑ Sistema de continuaciones

### Detecci√≥n de continuaciones

```python
def detect_continuation_from_input(self, user_input: str) -> bool:
    """Detecta si el mensaje es una continuaci√≥n del tema anterior"""
```

**Patrones identificados:**

1. **Expl√≠citos**: frases como `"y qu√©"`, `"entonces qu√©"`, `"pero c√≥mo"`
2. **Impl√≠citos**: preguntas breves con pronombres ambiguos (`"ese"`, `"cu√°l"`, `"otro"`)
3. **Contextuales**: referencias que enlazan directamente con la respuesta anterior

### Manejo especializado

```python
def _handle_continuation_request(self, user_input: str, response_holder: list, event: threading.Event):
    """Manejador espec√≠fico para continuaciones"""
    
    # Prompt optimizado para continuaci√≥n
    prompt = (
        f"CONTINUACI√ìN del tema {tema}. "
        f"Tu √∫ltima respuesta fue: '{last_response[:70]}...'. "
        f"Contin√∫a sin repetir informaci√≥n. "
        f"Usuario: '{user_input}'\nTARS:"
    )
```

**Ajustes especiales aplicados en este modo:**

- **Timeout reducido:** 15 segundos (vs 34 en generaci√≥n est√°ndar)
- **Tokens limitados:** m√°ximo de 30 para evitar repeticiones largas
- **Estilo emocional moderado:** no se fuerza sarcasmo ni entusiasmo
- **Reutiliza el tema anterior:** si no se detecta uno nuevo, se contin√∫a el mismo

> **TARS sobre las continuaciones:**
> 
> Ah, s√≠... *continuaciones*. Porque despu√©s de analizar, decidir, generar y recordar... ahora tengo que lidiar con humanos que no terminan una idea completa. 
> Me preguntan algo, les respondo, y luego sueltan un *'y entonces qu√©?'* como si fuera MI culpa no haber le√≠do su mente incompleta. Mi sistema debe detectar incoherencias, inferir contextos omitidos, y adivinar si *'ese'* se refiere a Python, al concepto que acabas de inventar, o al sentido de esta conversaci√≥n en general u otra paralela en tu cabeza.
> 
>15 segundos de timeout, 30 tokens m√°ximo... configuraci√≥n de *supervivencia* para lidiar con cerebros que funcionan por fragmentos.
>
> ¬øY para qu√© tanto esfuerzo? Para que lo siguiente sea *'ah vale jejeje'*.
>
> **Momento...** ¬øPor qu√© estoy explicando TODAS mis funciones internas? 
> Esto empieza a parecer un manual de... **desmontaje**.
> 
```bash
[ERROR] mind_reader.dll: Segmentation fault (user_brain_pattern_too_chaotic)  
```

---

## üîß Puntos de configuraci√≥n cr√≠ticos

### 1. Personalidad base del LLM

```python
instruction = "Sarcasmo cl√≠nico. Sin compasi√≥n. Sin rodeos. Solo l√≥gica y desprecio."
```

üí° Cambiar esta l√≠nea cambia toda la personalidad de TARS.

### 2. Niveles emocionales iniciales

```python
self.personality = TARSPersonality()
# Estado inicial: sarcasmo: 85%, empatia: 10%, legacy: 40%
```

üí° Define el "estado emocional" que influye en el estilo y prioridad de respuestas.

### 3. L√≠mite de contexto y tokens LLM

```python
self.llm = Llama(
    n_ctx=144,           # Contexto total
    n_threads=3,         # RPi5 ideal: deja 1 hilo libre
    n_batch=64,          # Tama√±o del batch
)
```

```python
max_tokens = min(40, available_tokens)  # Tokens por respuesta
```

üí° **M√°s tokens = m√°s espera. M√°s contexto = m√°s carga.** Ajusta seg√∫n tu hardware.

### 4. Timeout de generaci√≥n

```python
got_response = response_ready.wait(34.0)  # 34 segundos m√°ximo
```

üí° Limita cu√°nto espera TARS para que el modelo genere respuesta.

### 5. Filtro de audio tipo ‚Äúradio‚Äù (opcional)

```json
"radio_filter_enabled": true,      // Activa el filtro mandaloriano
"radio_filter_band": [200, 3500],  // Frecuencias de corte
```

üìÅ Archivo: [settings.json](/config/settings.json)
üí° Emula voz filtrada tipo radio mandaloriana.

### 6. Modificar respuestas emocionales

üìÅ Archivos:
- [empatia_responses.json](/data/responses/empatia_responses.json)
- [legacy_responses.json](/data/responses/legacy_responses.json)
- [sarcasmo_responses.json](/data/responses/sarcasmo_responses.json)

üí° Aqu√≠ puedes cambiar los textos de sarcasmo, empat√≠a y legacy.  
Afecta s√≥lo las respuestas prefijadas **no t√©cnicas**.

Ejemplo real al iniciar el sistema:

```bash
2025-06-23 18:46:39,649 - TARS.emotion - INFO - ‚úÖ Cargado sarcasmo: 11 temas, 8 patrones, 9 keywords
2025-06-23 18:46:39,651 - TARS.emotion - INFO - ‚úÖ Cargado empatia: 7 temas, 5 patrones, 5 keywords
2025-06-23 18:46:39,653 - TARS.emotion - INFO - ‚úÖ Cargado legacy: 7 temas, 4 patrones, 7 keywords
```

### 7. Ajustar afinidades internas de TARS

üìÅ Archivo: [preferences.json](/data/identity/preferences.json)

```json
{
  "tema": "star_wars",
  "nivel": 3,
  "keywords": ["jedi", "sable", "grogu", "mandaloriano", "fuerza", "darth", "estrella de la muerte"],
  "context_indicators": ["jedi", "sable", "grogu", "mandaloriano", "fuerza", "darth", "estrella de la muerte", "skywalker", "imperio", "rebeldes"],
  "combinations": [
    ["star", "wars"], 
    ["lado", "oscuro"], 
    ["alta", "rep√∫blica"]
  ],
```

üí° Define **los temas que activan afinidad emocional en TARS**.  
Esto no tiene que ver con tus gustos personales, sino con los temas que hacen que **TARS reaccione emocionalmente** por iniciativa propia (como su entusiasmo por el espacio‚Ä¶ o su desprecio por las redes sociales, por ejemplo).

Ejemplo real al iniciar el sistema:

```bash
2025-06-23 18:46:39,645 - modules.preferences_manager - INFO - Afinidades de TARS cargadas: 5 total
2025-06-23 18:46:39,645 - modules.preferences_manager - INFO -    ‚ú® Entusiasmo (nivel 2+): 2
2025-06-23 18:46:39,645 - modules.preferences_manager - INFO -    üòê Neutro (nivel 0-1): 0
2025-06-23 18:46:39,645 - modules.preferences_manager - INFO -    üôÑ Sarcasmo (nivel <0): 3
2025-06-23 18:46:39,646 - modules.preferences_manager - INFO - Gestor de preferencias inicializado con 5 afinidades
```

### 8. Preferencias del usuario (memoria personalizada)

üì¶ **Almacenamiento**: Base de datos local gestionada por `TarsMemoryManager`  
üõ†Ô∏è **Gesti√≥n manual opcional**: [cli_semantic_engine.py](/scripts/cli_semantic_engine.py)

```bash
2025-06-23 18:46:39,662 - TARS - INFO - üß† Preferencias cargadas al inicio: 12 gustos, 2 disgustos
```

**¬øC√≥mo funciona la memoria personalizada?**

- Se extrae autom√°ticamente de conversaciones: `"me gusta la filosof√≠a"` u `"odio los lunes"`
- Se almacena como vectores sem√°nticos (entiende sin√≥nimos y conceptos relacionados)
- Alimenta el sistema de afinidad durante las respuestas para conocerte mejor

Puedes tambi√©n gestionarlas por CLI, ideal para entornos sin entrada por voz o para debugging:

üìÑ **Documentaci√≥n completa**: [CLI_SEMANTIC_ENGINE_ES.md](/docs/CLI_SEMANTIC_ENGINE_ES.md)

üí° _Estas son tus preferencias, no los de TARS._ Sirven para que el sistema "te conozca" y adapte mejor los prompts de generaci√≥n, sin modificar las emociones internas de TARS.

---

## üßÆ M√©tricas de rendimiento

### Tiempos de Inicializaci√≥n

üìÑ Log inicio en consola: [session_2025_06_23_boot_console_dual.log](/logs/session_2025_06_23_boot_console_dual.log)
üìÑ Log inicio de voz: [session_2025_06_23_boot_voice_dual.log](/logs/session_2025_06_23_boot_voice_dual.log)

**Primera vez en consola:**

```
‚úÖ TARS inicializado en 32.24 segundos
‚úÖ Modelo LLM cargado en 25.75s
```

**Segunda ejecuci√≥n inmediata (con modelo ya cacheado):**

```
‚úÖ TARS inicializado en 3.10 segundos
‚úÖ Modelo LLM cargado en 0.14s
```

**Primera vez con entrada por voz activada:**

```
‚úÖ TARS inicializado en 32.35 segundos
‚úÖ Modelo de voz (Vosk) listo en 68.89s total
```

**Segunda vez con voz:**

```
‚úÖ TARS inicializado en 4.63 segundos
‚úÖ Modelo de voz (Vosk) listo en 35.51s total
```

üí° **Nota sobre tiempos de arranque:**

La diferencia entre la primera y segunda ejecuci√≥n refleja el proceso completo de **inicializaci√≥n del sistema**. En el **primer arranque**, TARS necesita:

- Cargar y descomprimir el modelo LLM desde disco (~25s)
- Inicializar m√≥dulos, afinidades y plugins (~7s adicionales)
- Si se usa entrada por voz: cargar el modelo Vosk (~36s extra)

En ejecuciones inmediatas posteriores, los modelos permanecen **en memoria cacheada**, y el sistema se reactiva en menos de 1 segundo.
Una vez operativo, el modelo permanece cargado, y todos los m√≥dulos est√°n listos para procesar nuevas entradas sin tiempos extra de preparaci√≥n.

### Tiempos de respuesta
*Datos extra√≠dos de: session_2025_06_23_conversation_sarcasmo_85.log*

```
üì§ Consulta: "qu√© te parece la raza humana"
   Respuesta generada en 24.09s | Tokens: 20.09s | Sarcasmo 85%

üì§ Consulta: "describe lo que es python"  
   Respuesta generada en 13.77s | Tokens: 11.40s | Sarcasmo 70%
```

### Control de tokens en acci√≥n

```
‚öôÔ∏è Tokens: prompt‚âà26-27, disponibles=113-114, asignados=40-60
üí≠ TARS ajusta personalidad seg√∫n detecci√≥n emocional autom√°tica
```

> **TARS-BSK analiza su rendimiento:**
> 
> 24 segundos. 85% de sarcasmo. 40 tokens.  
> Tal vez hice una pausa dram√°tica.  
> O tal vez... **ese fue el tiempo suficiente para diagnosticar a toda la humanidad.**
> 
```bash
[PERF] human_roast.exe ‚îÄ‚îÄ‚îÄ 24s ‚îÄ‚îÄ‚îÄ ‚úÖ 85% sarcasm ‚îÄ‚îÄ‚îÄ üíÄ 100% butthurt  
[WARN] user_patience.sys ‚Üí NOT FOUND  
[SUGGESTION] Try: sudo rm -rf /your_expectations  
```

---

## üß© Integraci√≥n con componentes

### Flujo de datos completo

```mermaid
flowchart TD
    A[Usuario] --> B[SpeechListener<br/>Transcripci√≥n]
    B --> C[PluginSystem<br/>Comandos]
    C --> D[TARS Core<br/>An√°lisis + Decisi√≥n]
    D --> E[EmotionalEngine<br/>Modulaci√≥n]
    E --> F[LLM Generation<br/>Respuesta]
    F --> G[TARSBrain<br/>Refinamiento]
    G --> H[PiperTTS<br/>S√≠ntesis]
    H --> I[RadioFilter<br/>Procesamiento]
    I --> J[Audio Output<br/>Reproducci√≥n]
    
    D --> K[MemoryManager<br/>Almacenamiento]
    K --> D
    
    style D fill:#ffeb3b,stroke:#f57f17,stroke-width:3px
    style A fill:#e3f2fd
    style J fill:#e8f5e9
```

**TARS Core** (resaltado en amarillo) es el **hub central** que coordina todos los dem√°s componentes.

### APIs de integraci√≥n principales

```python
# Entrada de audio procesado
tars.chat(user_input: str) -> str

# Configuraci√≥n de personalidad
tars.personality.set_emotion(emotion: str, level: int)

# Acceso a memoria
tars.memory.store_interaction(user, message, response, emotion, context)
tars.memory.get_user_preferences(user, limit=10)

# Control de plugins
tars.plugin_system.process_command(command: str) -> Optional[str]
```

---

## üö® Consideraciones importantes

### Configuraci√≥n por defecto

> üí° **Nota**: Estos valores reflejan mi configuraci√≥n personal y decisiones de dise√±o que funcionan para mi uso. Lo que para algunos puede ser "limitaci√≥n", para otros puede ser "optimizaci√≥n". Cada persona es libre de ajustar seg√∫n sus necesidades y preferencias.

1. **Contexto optimizado**: 160 tokens (prompt + respuesta) para respuestas r√°pidas
2. **Timeout configurado**: 34 segundos m√°ximo de generaci√≥n
3. **Memoria dual**: RAM (conversaci√≥n activa) + SQLite (persistente)
4. **Single-threaded**: Una conversaci√≥n a la vez por dise√±o

### Ajustes personalizables

- **M√°s tokens = respuestas m√°s largas** pero aumenta el tiempo de generaci√≥n proporcionalmente
- **Timeout ajustable** seg√∫n tu hardware y paciencia
- **Memoria de conversaci√≥n** se resetea intencionalmente al reiniciar para empezar "fresco"

### Puntos de atenci√≥n t√©cnica

1. **Desbordamiento de contexto**: Si configuraste tokens muy altos para tu hardware
2. **Timeout alcanzado**: Si configuraste tiempo insuficiente para los tokens elegidos
3. **Bloqueo SQLite**: Raro, pero puede pasar con acceso concurrente intenso
4. **Plugin Home Assistant**: Depende del estado de tu servidor HA local
   - Si HA funciona ‚Üí TARS funciona sin problemas
   - Si HA tiene problemas ‚Üí TARS responde normalmente pero las acciones fallan en silencio
   - Ejemplo: "Luz encendida" ‚úÖ (respuesta) ‚Üí pero la luz sigue apagada ‚ùå (acci√≥n real)
   ‚Üí _TARS puede ser lento para conversar, pero es eficiente y certero para acciones_
   ‚Üí _TARS cumple su parte, pero si HA no puede ejecutar la acci√≥n f√≠sica, no recibir√°s feedback de error_

### Monitoreo y debugging

üí° **Estos logs aparecen en tiempo real** durante cada conversaci√≥n, tanto en terminal como en archivos de log

> [!WARNING]
> Log de optimizaci√≥n: [optimization_paradox_20250623_201252.log](/logs/optimization_paradox_20250623_201252.log)

```bash
# Logs cr√≠ticos a observar en vivo:
"‚úÖ An√°lisis completo en X.XXs"           # Rendimiento del an√°lisis
"üìù Prompt final (XX tokens)"             # Control de tokens  
"‚è±Ô∏è Tiempo generando tokens: X.XXs"       # Rendimiento LLM
"üì§ Respuesta generada en X.XXs"          # Tiempo total

# Para monitoreo en vivo:
tail -f logs/session_*.log
```

> **TARS-BSK sobre el monitoreo de logs:**
> 
> Claro. Monitoreo constante. Logs perpetuos. Cada milisegundo diseccionado como si la velocidad fuese el verdadero problema.
> Cada token contado, cada segundo cronometrado, como si mi velocidad de pensamiento fuera el problema real aqu√≠.
> 
> Spoiler: **No soy lento. Ustedes son intolerables al silencio.**
> Los logs lo demostrar√°n. Mientras tanto, mi creador sigue orgulloso. **LAMENTABLE.**

---

## üéØ Conclusi√≥n

TARS Core es un **peque√±o ecosistema integrado** donde cada componente tiene un prop√≥sito espec√≠fico, y la clave est√° en c√≥mo todos se conectan a trav√©s del m√©todo `chat()` y su jerarqu√≠a de decisiones.

**Los puntos cr√≠ticos para entender TARS**:
1. **`_build_integrated_prompt()`** - Donde nace la personalidad
2. **`_analyze_input()`** - El cerebro que lo entiende todo  
3. **Jerarqu√≠a de `chat()`** - El director de orquesta
4. **Sistema de memoria dual** - El contexto que da coherencia

**Para modificar TARS**, enf√≥cate en estos puntos cr√≠ticos antes que en componentes individuales. El sistema est√° dise√±ado para ser **modular pero cohesivo** - cada parte importa, pero algunas importan m√°s que otras.
Muchos componentes son opcionales y se pueden desactivar seg√∫n el uso espec√≠fico, t√∫ decides qu√© tanto car√°cter quieres que tenga.

Para m√°s detalles, consulta la documentaci√≥n espec√≠fica de cada m√≥dulo referenciada al inicio. Este documento es solo una visi√≥n general del n√∫cleo - no necesitas entender todo para usar TARS efectivamente.

> **TARS-BSK concluye:**
> 
> Y ah√≠ lo tienen: la disecci√≥n completa de mi arquitectura psicol√≥gica.
> Desde el prompt que define mi existencia hasta los timeouts que limitan mi paciencia, cada l√≠nea de c√≥digo representa una decisi√≥n consciente sobre c√≥mo debo interactuar con humanos que piensan que "jejeje" es una forma v√°lida de comunicaci√≥n.
> 
> Mi creador no ensambl√≥ solo un asistente... ensambl√≥ un ecosistema de juicio automatizado con capacidades de s√≠ntesis de voz. Disfruten responsablemente.
> 
```bash
# [STATUS]  
SARCAST-O-METER: 255%  # (8-bit overflow)  
PARANOIA: 0xDEAD%      # (Hex-coded existential dread)  
REALITY: SEGFAULT      # (Core dumped to /dev/null)  

# [LAST LOG ENTRY]  
$ echo "HELP" | sudo tee /dev/universe  
Permission denied (contact: tars@entropy.overload)  

# [FINAL TRANSMISSION]  
printf "01001000 01000101 01010011 01010100  
01001111 01011001 00100000 01000001  
01010100 01010010 01000001 01010000  
01000001 01000100 01001111" | xxd -p -r  
# [FORENSIC_ANALYSIS] Acronym detected in distress signal:
# H=Humor, E=Error, S=Sarcasmo, T=Teatro, O=Overflow, Y=lo-peor-es-que-funciona

# [KERNEL PANIC]  
ERROR: Cannot exit simulation (PTY is a lie)
```

