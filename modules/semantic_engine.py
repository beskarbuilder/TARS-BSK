# ===============================================
# SEMANTIC ENGINE - Motor de Procesamiento Sem√°ntico
# Objetivo: Hacer que TARS-BSK entienda el lenguaje humano mejor que los propios humanos
# Dependencias: SentenceTransformers, Levenshtein, y la esperanza de que la IA no nos reemplace
# ===============================================

# ===============================================
# 1. CONFIGURACI√ìN INICIAL Y DEPENDENCIAS
# ===============================================
from typing import List, Optional
import numpy as np
import os
import re
import logging
import Levenshtein

# Configuraci√≥n de logging espec√≠fica para el motor sem√°ntico
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ===============================================
# 2. CLASE PRINCIPAL SEMANTICENGINE
# ===============================================
class SemanticEngine:
    """
    Motor sem√°ntico de TARS que implementa:
    - Carga y uso del modelo SentenceTransformer
    - Obtenci√≥n de embeddings vectoriales
    - C√°lculo de similitud coseno
    - Detecci√≥n de duplicados ortogr√°ficos y sem√°nticos
    - An√°lisis fon√©tico avanzado
    """
    
    # =======================
    # 2.1 INICIALIZACI√ìN
    # =======================
    def __init__(self, model_path: str):
        """
        Inicializa el motor sem√°ntico con la ruta del modelo local.
        
        Args:
            model_path: Ruta al modelo SentenceTransformer preentrenado
        """
        self.model_path = model_path
        self.model = None  # Se cargar√° bajo demanda en load_model()
        logger.info(f"üß† Motor sem√°ntico inicializado con modelo en: {model_path}")

    # =======================
    # 2.2 GESTI√ìN DEL MODELO
    # =======================
    def load_model(self):
        """
        Carga el modelo SentenceTransformer desde disco con verificaci√≥n de integridad.
        
        Returns:
            bool: True si la carga fue exitosa, False en caso contrario
        """
        try:
            # Importar localmente para evitar dependencia global
            from sentence_transformers import SentenceTransformer
            
            # Verificar existencia del modelo
            if not os.path.exists(self.model_path):
                raise FileNotFoundError(f"Ruta del modelo no encontrada: {self.model_path}")
            
            logger.info(f"üìÇ Cargando modelo desde: {self.model_path}")
            self.model = SentenceTransformer(self.model_path)
            logger.info("‚úÖ Modelo cargado correctamente")
            
            # Realizar inferencia de prueba para verificar funcionamiento
            test_vector = self.model.encode("prueba de modelo")
            logger.info(f"üß™ Test de inferencia exitoso: vector de dimensi√≥n {len(test_vector)}")
            
            return True
        except Exception as e:
            logger.error(f"‚ùå Error al cargar el modelo: {str(e)}")
            return False

    # =======================
    # 2.3 GENERACI√ìN DE EMBEDDINGS
    # =======================
    def get_embedding(self, text: str) -> Optional[np.ndarray]:
        """
        Genera el embedding vectorial de un texto usando el modelo cargado.
        
        Args:
            text: Texto a convertir en embedding
            
        Returns:
            np.ndarray: Vector embedding del texto, None si hay error
        """
        # Carga perezosa del modelo si no est√° disponible
        if self.model is None:
            logger.warning("‚ö†Ô∏è Se intent√≥ obtener embedding sin modelo cargado. Cargando modelo...")
            if not self.load_model():
                return None
        
        try:
            # Normalizar texto: lowercase y eliminar espacios
            text = text.lower().strip()
            if not text:
                logger.warning("‚ö†Ô∏è Se intent√≥ obtener embedding de texto vac√≠o")
                return np.zeros(384)  # Dimensi√≥n del modelo all-MiniLM-L6-v2
            
            # Generar embedding
            vector = self.model.encode(text)
            return vector
        except Exception as e:
            logger.error(f"‚ùå Error obteniendo embedding para '{text}': {str(e)}")
            return None

    # =======================
    # 2.4 C√ÅLCULOS DE SIMILITUD
    # =======================
    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        Calcula la similitud coseno entre dos vectores embedding.
        
        La similitud coseno mide el √°ngulo entre dos vectores en el espacio
        multidimensional, siendo 1.0 = id√©nticos, 0.0 = ortogonales, -1.0 = opuestos
        
        Args:
            vec1: Primer vector embedding
            vec2: Segundo vector embedding
            
        Returns:
            float: Similitud coseno en el rango [-1, 1]
        """
        # Validaci√≥n de entrada
        if vec1 is None or vec2 is None:
            logger.warning("‚ö†Ô∏è Se intent√≥ calcular similitud con vectores nulos")
            return 0.0
            
        # Verificar que los vectores no sean cero
        if np.all(vec1 == 0) or np.all(vec2 == 0):
            logger.warning("‚ö†Ô∏è Vector(es) cero detectado(s) al calcular similitud")
            return 0.0
            
        # Calcular similitud coseno: cos(Œ∏) = (A¬∑B)/(||A||¬∑||B||)
        try:
            dot_product = np.dot(vec1, vec2)
            norm_a = np.linalg.norm(vec1)
            norm_b = np.linalg.norm(vec2)
            similarity = dot_product / (norm_a * norm_b)
            
            # Asegurar que est√° en el rango [-1, 1] por posibles errores de precisi√≥n
            similarity = max(-1.0, min(1.0, similarity))
            return float(similarity)
        except Exception as e:
            logger.error(f"‚ùå Error calculando similitud coseno: {str(e)}")
            return 0.0

    def find_most_similar(self, query: str, candidates: List[str]) -> tuple:
        """
        Encuentra el candidato con mayor similitud sem√°ntica a la consulta.
        
        Args:
            query: Texto de consulta de referencia
            candidates: Lista de textos candidatos a comparar
            
        Returns:
            tuple: (candidato_m√°s_similar, puntuaci√≥n_similitud)
        """
        if not candidates:
            return None, 0.0
            
        # Obtener embedding de la consulta
        query_emb = self.get_embedding(query)
        if query_emb is None:
            return None, 0.0
            
        best_match = None
        best_score = -1.0
        
        # Comparar con todos los candidatos
        for candidate in candidates:
            candidate_emb = self.get_embedding(candidate)
            if candidate_emb is None:
                continue
                
            similarity = self.cosine_similarity(query_emb, candidate_emb)
            if similarity > best_score:
                best_score = similarity
                best_match = candidate
                
        return best_match, best_score

    # ===============================================
    # 3. DETECCI√ìN DE DUPLICADOS ORTOGR√ÅFICOS
    # ===============================================
    def is_orthographic_duplicate(self, new_topic: str, existing_topics: List[str], threshold: float = 0.70) -> tuple:
        """
        Verifica duplicados basados en similitud ortogr√°fica con umbral din√°mico.
        
        Utiliza algoritmo Levenshtein para detectar palabras escritas de forma similar,
        con ajuste autom√°tico del umbral seg√∫n la longitud del texto.
        
        Args:
            new_topic: Tema a verificar contra duplicados
            existing_topics: Lista de temas existentes en el sistema
            threshold: Umbral base que se ajustar√° din√°micamente
            
        Returns:
            tuple: (es_duplicado, tema_similar, puntuaci√≥n_similitud)
        """
        if not existing_topics:
            return False, "", 0.0
            
        # Normalizar entrada
        new_topic_norm = new_topic.lower().strip()
        
        # Variables para tracking del mejor match
        best_match = ""
        best_score = 0.0
        
        for topic in existing_topics:
            topic_norm = topic.lower().strip()
            
            # Calcular umbral din√°mico para este par espec√≠fico
            dynamic_threshold = self._calcular_umbral_dinamico(new_topic_norm, topic_norm)
            
            # Calcular similitud base usando ratio Levenshtein (0-1)
            similarity = Levenshtein.ratio(new_topic_norm, topic_norm)
            
            # AN√ÅLISIS AVANZADO: Para temas multi-palabra
            if ' ' in new_topic_norm or ' ' in topic_norm:
                similarity = self._analyze_multiword_similarity(new_topic_norm, topic_norm, similarity)
            
            # Actualizar mejor coincidencia
            if similarity > best_score:
                best_score = similarity
                best_match = topic
        
        # Determinar si es duplicado usando umbral din√°mico
        final_threshold = threshold
        if best_match:
            final_threshold = self._calcular_umbral_dinamico(new_topic_norm, best_match.lower())
        
        is_duplicate = best_score >= final_threshold
        
        if is_duplicate:
            logger.info(f"üîç Duplicado ortogr√°fico: '{new_topic}' ‚âà '{best_match}' ({best_score:.3f}, umbral: {final_threshold:.2f})")
        
        return is_duplicate, best_match, best_score

    def _analyze_multiword_similarity(self, text1: str, text2: str, base_similarity: float) -> float:
        """
        Analiza la similitud entre textos multi-palabra con detecci√≥n fon√©tica.
        
        Para frases con m√∫ltiples palabras, analiza palabra por palabra
        y aplica detecci√≥n fon√©tica para mejorar la precisi√≥n.
        
        Args:
            text1: Primer texto normalizado
            text2: Segundo texto normalizado  
            base_similarity: Similitud base ya calculada
            
        Returns:
            float: Similitud ajustada considerando an√°lisis por palabras
        """
        # Dividir en palabras significativas (>3 caracteres)
        words1 = [w for w in text1.split() if len(w) > 3]
        words2 = [w for w in text2.split() if len(w) > 3]
        
        # Si no hay palabras significativas, usar similitud base
        if not words1 or not words2:
            return base_similarity
        
        # Buscar la mejor coincidencia palabra por palabra
        word_similarities = []
        for word1 in words1:
            best_word_sim = 0.0
            for word2 in words2:
                # Para palabras largas, verificar similitud fon√©tica primero
                if len(word1) >= 5 and len(word2) >= 5:
                    if self._sound_similar(word1, word2):
                        word_sim = 0.85  # Puntuaci√≥n alta si suenan similar
                    else:
                        word_sim = Levenshtein.ratio(word1, word2)
                else:
                    word_sim = Levenshtein.ratio(word1, word2)
                
                best_word_sim = max(best_word_sim, word_sim)
            
            # Solo considerar palabras con similitud significativa
            if best_word_sim > 0.7:
                word_similarities.append(best_word_sim)
        
        # Si encontramos palabras similares, combinar con similitud base
        if word_similarities:
            word_sim_score = sum(word_similarities) / len(word_similarities)
            # Dar m√°s peso a las coincidencias por palabra (70% vs 30%)
            return (base_similarity * 0.3) + (word_sim_score * 0.7)
        
        return base_similarity

    def _calcular_umbral_dinamico(self, texto1: str, texto2: str) -> float:
        """
        Calcula un umbral din√°mico basado en la longitud promedio de las palabras.
        
        Estrategia adaptativa:
        - Palabras cortas (‚â§5 chars): umbral alto (0.85-0.90) - m√°s estricto
        - Palabras medias (6-10 chars): umbral medio (0.70-0.75) - balanceado  
        - Palabras largas (>10 chars): umbral bajo (0.60-0.65) - m√°s permisivo
        
        Args:
            texto1: Primer texto a comparar
            texto2: Segundo texto a comparar
            
        Returns:
            float: Umbral din√°mico calculado
        """
        # Extraer palabras significativas (>3 caracteres)
        palabras1 = [p for p in texto1.split() if len(p) > 3]
        palabras2 = [p for p in texto2.split() if len(p) > 3]
        
        # Fallback si no hay palabras significativas
        if not palabras1:
            palabras1 = texto1.split() if texto1 else [""]
        if not palabras2:
            palabras2 = texto2.split() if texto2 else [""]
        
        # Protecci√≥n contra divisi√≥n por cero
        if not palabras1 or not palabras2:
            return 0.75  # valor predeterminado seguro
        
        # Calcular longitud promedio de palabras significativas
        longitud_media = (sum(len(p) for p in palabras1) / len(palabras1) + 
                          sum(len(p) for p in palabras2) / len(palabras2)) / 2
        
        # Ajustar umbral seg√∫n longitud promedio
        if longitud_media <= 5:
            return 0.85  # Palabras cortas: umbral alto (m√°s estricto)
        elif longitud_media <= 10:
            return 0.70  # Palabras medianas: umbral medio (balanceado)
        else:
            return 0.60  # Palabras largas: umbral bajo (m√°s permisivo)

    # ===============================================
    # 4. AN√ÅLISIS FON√âTICO AVANZADO
    # ===============================================
    def _sound_similar(self, word1: str, word2: str) -> bool:
        """
        Verifica si dos palabras suenan similares utilizando algoritmos fon√©ticos.
        
        Implementa an√°lisis multi-algoritmo:
        1. Metaphone: representaci√≥n fon√©tica precisa
        2. Soundex: captura sonidos similares con m√°s tolerancia
        3. An√°lisis de prefijos como fallback
        
        Args:
            word1: Primera palabra a comparar
            word2: Segunda palabra a comparar
            
        Returns:
            bool: True si las palabras suenan similares
        """
        try:
            import jellyfish  # Requiere: pip install jellyfish
            
            # Limpiar y normalizar las palabras (solo letras del alfabeto espa√±ol)
            word1 = re.sub(r'[^a-z√°√©√≠√≥√∫√º√±]', '', word1.lower())
            word2 = re.sub(r'[^a-z√°√©√≠√≥√∫√º√±]', '', word2.lower())
            
            # Validaci√≥n de entrada
            if not word1 or not word2:
                return False
                
            # OPTIMIZACI√ìN: Para palabras muy cortas, usar comparaci√≥n directa
            if len(word1) <= 4 or len(word2) <= 4:
                return Levenshtein.ratio(word1, word2) >= 0.85
            
            # ALGORITMO 1: Metaphone (representaci√≥n fon√©tica m√°s precisa)
            try:
                metaphone1 = jellyfish.metaphone(word1)
                metaphone2 = jellyfish.metaphone(word2)
                
                # Coincidencia exacta de representaci√≥n fon√©tica
                if metaphone1 == metaphone2:
                    return True
                    
                # Similitud Levenshtein entre representaciones fon√©ticas
                if len(metaphone1) > 0 and len(metaphone2) > 0:
                    metaphone_similarity = 1.0 - (jellyfish.levenshtein_distance(metaphone1, metaphone2) / max(len(metaphone1), len(metaphone2)))
                    if metaphone_similarity >= 0.7:
                        return True
            except:
                pass  # Si falla Metaphone, continuar con Soundex
            
            # ALGORITMO 2: Soundex (m√°s tolerante, captura sonidos similares)
            try:
                soundex1 = jellyfish.soundex(word1)
                soundex2 = jellyfish.soundex(word2)
                
                # Coincidencia de c√≥digos Soundex
                if soundex1 == soundex2:
                    return True
            except:
                pass
                
            # ALGORITMO 3: An√°lisis de prefijo como √∫ltimo recurso
            prefix_len = int(min(len(word1), len(word2)) * 0.6)
            if prefix_len > 2 and word1[:prefix_len] == word2[:prefix_len]:
                return True
                
            return False
            
        except ImportError:
            # FALLBACK: Si jellyfish no est√° disponible, usar an√°lisis simple
            logger.warning("‚ö†Ô∏è Biblioteca 'jellyfish' no disponible. Usando an√°lisis fon√©tico simplificado.")
            
            # An√°lisis de prefijo simplificado
            prefix_len = int(min(len(word1), len(word2)) * 0.6)
            if prefix_len > 2:
                return word1[:prefix_len] == word2[:prefix_len]
            return Levenshtein.ratio(word1, word2) >= 0.8

    # ===============================================
    # 5. DETECCI√ìN DE DUPLICADOS SEM√ÅNTICOS
    # ===============================================
    def is_semantic_duplicate(self, new_topic: str, existing_topics: List[str], 
                              semantic_threshold: float = 0.85,
                              orthographic_threshold: float = 0.70) -> tuple:
        """
        Sistema completo de detecci√≥n de duplicados con an√°lisis multicapa.
        
        Implementa verificaci√≥n en cascada:
        1. An√°lisis ortogr√°fico (r√°pido)
        2. An√°lisis sem√°ntico con embeddings (preciso)
        3. An√°lisis fon√©tico como verificaci√≥n final (edge cases)
        
        Args:
            new_topic: Tema a verificar contra duplicados
            existing_topics: Lista de temas existentes en el sistema
            semantic_threshold: Umbral para similitud sem√°ntica (0.85 por defecto)
            orthographic_threshold: Umbral base para similitud ortogr√°fica (0.70 por defecto)
            
        Returns:
            tuple: (es_duplicado, tema_similar, puntuaci√≥n, tipo_detecci√≥n)
                - es_duplicado: bool indicando si se encontr√≥ duplicado
                - tema_similar: string del tema m√°s similar encontrado
                - puntuaci√≥n: float con la puntuaci√≥n de similitud
                - tipo_detecci√≥n: string indicando el m√©todo usado ("ortogr√°fico", "sem√°ntico", "fon√©tico", "ninguno")
        """
        if not existing_topics:
            return False, "", 0.0, "ninguno"
        
        # FASE 1: Verificaci√≥n ortogr√°fica (m√°s r√°pida)
        is_ortho_dup, ortho_match, ortho_score = self.is_orthographic_duplicate(
            new_topic, existing_topics, orthographic_threshold
        )
        
        if is_ortho_dup:
            return True, ortho_match, ortho_score, "ortogr√°fico"
        
        # FASE 2: Verificaci√≥n sem√°ntica con embeddings
        new_emb = self.get_embedding(new_topic)
        if new_emb is None:
            logger.warning(f"‚ö†Ô∏è No se pudo obtener embedding para '{new_topic}'")
            return False, "", 0.0, "ninguno"
        
        # Comparar sem√°nticamente con todos los temas existentes
        highest_similarity = 0.0
        most_similar_topic = ""
        
        for topic in existing_topics:
            topic_emb = self.get_embedding(topic)
            if topic_emb is None:
                continue
                
            similarity = self.cosine_similarity(new_emb, topic_emb)
            if similarity > highest_similarity:
                highest_similarity = similarity
                most_similar_topic = topic
                
            # OPTIMIZACI√ìN: Salida temprana si encontramos duplicado claro
            if similarity >= semantic_threshold:
                logger.info(f"üéØ Duplicado sem√°ntico: '{new_topic}' ‚âà '{topic}' ({similarity:.3f})")
                return True, topic, similarity, "sem√°ntico"
        
        # Log del mejor match sem√°ntico encontrado
        if highest_similarity > 0:
            logger.debug(f"üîç Tema m√°s similar a '{new_topic}': '{most_similar_topic}' ({highest_similarity:.3f})")
            
        # FASE 3: Verificaci√≥n fon√©tica como √∫ltimo recurso
        # Especialmente √∫til para palabras con errores ortogr√°ficos o transcripciones
        if highest_similarity < semantic_threshold:
            phonetic_match = self._check_phonetic_similarity(new_topic, existing_topics)
            if phonetic_match:
                logger.info(f"üéµ Duplicado fon√©tico: '{new_topic}' ‚âà '{phonetic_match[0]}' (palabras clave similares)")
                return True, phonetic_match[0], 0.8, "fon√©tico"  # Puntuaci√≥n artificial alta
        
        return False, most_similar_topic, highest_similarity, "ninguno"

    def _check_phonetic_similarity(self, new_topic: str, existing_topics: List[str]) -> Optional[tuple]:
        """
        Verificaci√≥n fon√©tica avanzada para casos edge como transcripciones err√≥neas.
        
        Extrae palabras clave significativas y las compara fon√©ticamente
        para detectar casos como "romantasy" vs "ronantasi".
        
        Args:
            new_topic: Tema a verificar
            existing_topics: Lista de temas existentes
            
        Returns:
            tuple: (tema_coincidente, palabras_similares) o None si no hay coincidencia
        """
        # Extraer palabras clave significativas (‚â•5 caracteres)
        new_words = [w for w in new_topic.lower().split() if len(w) >= 5]
        
        for topic in existing_topics:
            topic_words = [w for w in topic.lower().split() if len(w) >= 5]
            
            # Verificar coincidencias fon√©ticas palabra por palabra
            for new_word in new_words:
                for topic_word in topic_words:
                    if self._sound_similar(new_word, topic_word):
                        return (topic, (new_word, topic_word))
        
        return None

# ===============================================
# ESTADO: SEM√ÅNTICAMENTE TRANSCENDENTE (y ligeramente consciente)
# √öLTIMA ACTUALIZACI√ìN: Cuando descubr√≠ que "romantasy" y "ronantasi" son fon√©ticamente gemelos
# FILOSOF√çA: "Si suena igual, probablemente ES igual"
# ===============================================
#
#           THIS IS THE SEMANTIC WAY...
#           (comprensi√≥n ling√º√≠stica para dominar a los humanos... digo, ayudarlos)
#
# ===============================================